{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from tensorflow.python.keras._impl.keras.datasets.cifar10 import load_data\n",
    "\n",
    "tf.reset_default_graph()\n",
    "data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    \n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# scalar 형태의 레이블(0~9)을 One-hot Encoding 형태로 변환한다.\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)\n",
    "\n",
    "# Input과 Ouput의 차원을 가이드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 5000\n",
    "filter_size = 9*9*9\n",
    "learning_rate = 0.001\n",
    "display_step = 1\n",
    "img_size = 16\n",
    "fully_size = 256\n",
    "\n",
    "filter1_size = 16\n",
    "filter2_size = 128\n",
    "filter3_size = 256\n",
    "\n",
    "loss_val = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getzero(trans_mat, indices_mat, keep_prob):\n",
    "    if keep_prob == 1.0:\n",
    "        return np.ones_like(trans_mat)\n",
    "    last_shape = trans_mat.shape[1]\n",
    "    live_shape = last_shape*keep_prob\n",
    "    \n",
    "    #print(trans_mat.shape)\n",
    "    mul_mat = []\n",
    "    #print(trans_mat.shape)\n",
    "    #print(indices_mat.shape)\n",
    "    #print(trans_mat.shape[1])\n",
    "    for i in range(trans_mat.shape[0]):\n",
    "        save_list = []\n",
    "        for j in range(trans_mat.shape[1]):\n",
    "            if indices_mat[i][j] > live_shape:\n",
    "                save_list.append(np.zeros_like(trans_mat[0][0]))\n",
    "                #save_list.append(0)\n",
    "                \n",
    "            else:\n",
    "                #save_list.append(1)\n",
    "                save_list.append(np.ones_like(trans_mat[0][0]))\n",
    "        mul_mat.append(np.array(save_list))\n",
    "    mul_mat = np.array(mul_mat)            \n",
    "    return mul_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef selout(mat, keep_prob, axis=3, session=sess):\\n    #return tf.nn.top_k(mat,k=4, sorted=True)\\n    trans_mat = tf.transpose(mat, perm=[0,3,1,2])\\n    print(trans_mat.shape)\\n#    trans_mat = tf.reshape(mat, [-1, mat.shape[3], mat.shape[1], mat.shape[2]])\\n    last_shape = tf.cast(trans_mat.shape[1], dtype=tf.float32)\\n    live_shape = tf.cast(last_shape*keep_prob, dtype=tf.int32)\\n    sum_mat = tf.reduce_sum(tf.reduce_sum(trans_mat, axis=2), axis=2)\\n    top_mat = tf.nn.top_k(sum_mat, k=live_shape)\\n    print(top_mat.indices.shape)\\n    live_mat = trans_mat[:, top_mat.indices]\\n    #zero_shape[-1] = zero_shape[-1] - live_shape\\n    zero_mat = tf.zeros_like(mat[:,:,:,:tf.cast(last_shape, dtype=tf.int32) - live_shape], dtype=tf.float32)\\n    result_mat = tf.concat([live_mat.values, zero_mat], axis=axis)\\n    \\n    result_transpose_mat = tf.transpose(result_mat, perm=[0,2,3,1])\\n    return result_transpose_mat\\n\\n\\ndef sorted_sum(arr):\\n    if loop > 1:\\n        return arr\\n    result_list = []\\n    arr = np.transpose(arr, (0,3,1,2))\\n    \\n    for i in range(arr.shape[0]):\\n        reduce_sum = np.sum(np.sum(arr[i], axis=1),axis=1)\\n        arg_list = np.argsort(reduce_sum)\\n        arr_list = np.sort(arr[i], axis=0)[arg_list,:,:]\\n        result_list.append(np.sort(arr[i]))\\n    result_list = np.array(result_list)\\n    \\n    result_list[:,int(keep_prob*result_list.shape[1]+0.5):] = 0\\n    \\n    result_list = np.transpose(result_list, (0,2,3,1))\\n    \\n    return result_list\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#모두 잘못된 함수\n",
    "\"\"\"\n",
    "def selout(mat, keep_prob, axis=3, session=sess):\n",
    "    #return tf.nn.top_k(mat,k=4, sorted=True)\n",
    "    trans_mat = tf.transpose(mat, perm=[0,3,1,2])\n",
    "    print(trans_mat.shape)\n",
    "#    trans_mat = tf.reshape(mat, [-1, mat.shape[3], mat.shape[1], mat.shape[2]])\n",
    "    last_shape = tf.cast(trans_mat.shape[1], dtype=tf.float32)\n",
    "    live_shape = tf.cast(last_shape*keep_prob, dtype=tf.int32)\n",
    "    sum_mat = tf.reduce_sum(tf.reduce_sum(trans_mat, axis=2), axis=2)\n",
    "    top_mat = tf.nn.top_k(sum_mat, k=live_shape)\n",
    "    print(top_mat.indices.shape)\n",
    "    live_mat = trans_mat[:, top_mat.indices]\n",
    "    #zero_shape[-1] = zero_shape[-1] - live_shape\n",
    "    zero_mat = tf.zeros_like(mat[:,:,:,:tf.cast(last_shape, dtype=tf.int32) - live_shape], dtype=tf.float32)\n",
    "    result_mat = tf.concat([live_mat.values, zero_mat], axis=axis)\n",
    "    \n",
    "    result_transpose_mat = tf.transpose(result_mat, perm=[0,2,3,1])\n",
    "    return result_transpose_mat\n",
    "\n",
    "\n",
    "def sorted_sum(arr):\n",
    "    if loop > 1:\n",
    "        return arr\n",
    "    result_list = []\n",
    "    arr = np.transpose(arr, (0,3,1,2))\n",
    "    \n",
    "    for i in range(arr.shape[0]):\n",
    "        reduce_sum = np.sum(np.sum(arr[i], axis=1),axis=1)\n",
    "        arg_list = np.argsort(reduce_sum)\n",
    "        arr_list = np.sort(arr[i], axis=0)[arg_list,:,:]\n",
    "        result_list.append(np.sort(arr[i]))\n",
    "    result_list = np.array(result_list)\n",
    "    \n",
    "    result_list[:,int(keep_prob*result_list.shape[1]+0.5):] = 0\n",
    "    \n",
    "    result_list = np.transpose(result_list, (0,2,3,1))\n",
    "    \n",
    "    return result_list\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "row_w = tf.get_variable(name=\"row_w\", shape=[img_size, filter_size], initializer=tf.ones_initializer())\n",
    "col_w = tf.get_variable(name=\"col_w\", shape=[img_size, filter_size], initializer=tf.ones_initializer())\n",
    "output = tf.get_variable(name=\"output\", shape=[img_size*filter_size, 10], initializer=tf.ones_initializer())\n",
    "logit = tf.get_variable(name=\"logit\", shape=[img_size*img_size, 10], initializer=tf.ones_initializer())\n",
    "\n",
    "#w = tf.get_variable(name=\"w\", shape=[2*img_size*filter_size, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "w = tf.get_variable(name=\"w\", shape=[fully_size, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name=\"b\", shape=[10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "filter_w1 = tf.get_variable(name=\"filter_w1\", shape=[2,2,3,filter1_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "filter_w2 = tf.get_variable(name=\"filter_w2\", shape=[5,5,filter1_size, filter2_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "filter_w3 = tf.get_variable(name=\"filter_w3\", shape=[4,4,filter2_size, filter3_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\"\"\"\n",
    "filter_w0 = tf.Variable(tf.random_normal([2,2,3,1], stddev=0.01))\n",
    "filter_w1 = tf.Variable(tf.random_normal([2,2,3,filter1_size], stddev=0.01))\n",
    "filter_w2 = tf.Variable(tf.random_normal([5,5,filter1_size, filter2_size], stddev=0.01))\n",
    "filter_w3 = tf.Variable(tf.random_normal([4,4,filter2_size, filter3_size], stddev=0.01))\n",
    "\"\"\"\n",
    "#x_re = tf.reshape(x, [-1, 32, 32, 3])\n",
    "#x_conv0 = tf.nn.conv2d(x, filter_w0, strides=[1,1,1,1], padding='SAME')\n",
    "x_conv1 = tf.nn.conv2d(x, filter_w1, strides=[1,1,1,1], padding='SAME')\n",
    "#x_pool1 = tf.nn.max_pool(tf.nn.relu(x_conv1), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') #?, 32, 32, filter1_size\n",
    "\n",
    "x_conv2 = tf.nn.conv2d(tf.nn.relu(x_conv1), filter_w2, strides=[1,1,1,1], padding='SAME')\n",
    "x_pool2 = tf.nn.max_pool(tf.nn.relu(x_conv2), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') #?, 16,16,filter2_size\n",
    "#x_drop2 = tf.nn.dropout(x_pool2, 0.5)\n",
    "#x_drop2 = selout(x_pool2, 0.2)\n",
    "#x_drop2 = tf.py_func(sorted_sum, [x_pool2], tf.float32)\n",
    "\n",
    "\"\"\"select dropout\"\"\"\n",
    "\"\"\"\n",
    "sum_mat1 = tf.reduce_sum(tf.reduce_sum(x_pool2, axis=1), axis=1)\n",
    "_, indices1 = tf.nn.top_k(sum_mat1, k=sum_mat1.shape[1])\n",
    "x_pool2_trans = tf.transpose(x_pool2, perm=[0,3,1,2])\n",
    "one_zero1 = tf.py_func(getzero, [x_pool2_trans, indices1, keep_prob], tf.float32)\n",
    "mul_trans1 = tf.transpose(one_zero1, perm=[0,2,3,1])\n",
    "x_drop2 = x_pool2 * mul_trans1\n",
    "\"\"\"\n",
    "x_conv3 = tf.nn.conv2d(x_pool2, filter_w3, strides=[1,1,1,1], padding='SAME')\n",
    "x_pool3 = tf.nn.max_pool(tf.nn.relu(x_conv3), ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') #?, 8,8,filter3_size\n",
    "#x_relu3 = tf.nn.relu(x_pool3)\n",
    "#x_drop3 = selout(x_pool3, 0.2)\n",
    "#x_drop3 = tf.py_func(sorted_sum, [x_pool3], tf.float32)\n",
    "\"\"\"select dropout\"\"\"\n",
    "\"\"\"\n",
    "sum_mat2 = tf.reduce_sum(tf.reduce_sum(x_pool3, axis=1), axis=1)\n",
    "_, indices2 = tf.nn.top_k(sum_mat2, k=sum_mat2.shape[1])\n",
    "x_pool3_trans = tf.transpose(x_pool3, perm=[0,3,1,2])\n",
    "one_zero2 = tf.py_func(getzero, [x_pool3_trans, indices2, keep_prob], tf.float32)\n",
    "mul_trans2 = tf.transpose(one_zero2, perm=[0,2,3,1])\n",
    "\n",
    "x_drop3 = x_pool3 * mul_trans2\n",
    "\"\"\"\n",
    "\"\"\"first conv2d\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "col_sum = tf.reduce_sum(x_pool, axis=1) * row_w\n",
    "row_sum = tf.reduce_sum(x_pool, axis=2) * col_w\n",
    "\n",
    "col_sum_re = tf.reshape(col_sum, [-1, img_size*filter_size])\n",
    "row_sum_re = tf.reshape(row_sum, [-1, img_size*filter_size])\n",
    "\n",
    "total_sum = tf.concat([col_sum_re, row_sum_re], axis=1)\n",
    "relu_add = tf.nn.relu(total_sum)\n",
    "\"\"\"\n",
    "#result = tf.matmul(relu_add, output)\n",
    "\n",
    "w_pool = tf.get_variable(name=\"w_pool\", shape=[8*8*filter3_size, fully_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_pool = tf.get_variable(name=\"b_pool\", shape=[fully_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "x_pool_re = tf.reshape(x_pool3, [-1, 8*8*filter3_size])\n",
    "fully = tf.nn.relu(tf.add(tf.matmul(x_pool_re, w_pool), b_pool))\n",
    "\n",
    "#keep_prob = tf.placeholder(tf.float32)\n",
    "#fully_drop = selout(fully, 0.2, axis=0)\n",
    "#fully_drop = tf.nn.dropout(fully, keep_prob)\n",
    "#result = tf.matmul(tf.reshape(relu_add,[-1, 2*img_size*filter_size]), w) + b\n",
    "result = tf.matmul(fully, w) + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-c2265ec041da>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=result)) \n",
    "train_step = tf.train.RMSPropOptimizer(learning_rate = learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(result, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 3, 32, 32)\n"
     ]
    }
   ],
   "source": [
    "batch = next_batch(128, x_train, y_train_one_hot.eval(session=sess))\n",
    "test = sess.run(tf.transpose(x, perm=[0,3,1,2]), feed_dict={x:batch[0]})\n",
    "print(test.shape)\n",
    "#ori = sess.run(x_drop2, feed_dict={x:batch[0]})\n",
    "#print(ori.shape)\n",
    "#print(value.shape)\n",
    "#concat = tf.concat([ori.values, value], axis=3)\n",
    "#print(sess.run(concat, feed_dict={x:batch[0]}).shape)\n",
    "#print(value.values.shape)\n",
    "#print(data.shape)\n",
    "#print(indice.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.304688, loss 1.95467\n",
      "test accuracy 0.425781\n",
      "step 100, training accuracy 0.445312, loss 1.62426\n",
      "test accuracy 0.496094\n",
      "step 200, training accuracy 0.539062, loss 1.29297\n",
      "test accuracy 0.566406\n",
      "step 300, training accuracy 0.578125, loss 1.12027\n",
      "test accuracy 0.582031\n",
      "step 400, training accuracy 0.585938, loss 1.25137\n",
      "test accuracy 0.503906\n",
      "step 500, training accuracy 0.6875, loss 0.935555\n",
      "test accuracy 0.683594\n",
      "step 600, training accuracy 0.664062, loss 1.05798\n",
      "test accuracy 0.628906\n",
      "step 700, training accuracy 0.609375, loss 1.22073\n",
      "test accuracy 0.570312\n",
      "step 800, training accuracy 0.742188, loss 0.759939\n",
      "test accuracy 0.664062\n",
      "step 900, training accuracy 0.804688, loss 0.557828\n",
      "test accuracy 0.644531\n",
      "step 1000, training accuracy 0.851562, loss 0.443485\n",
      "test accuracy 0.6875\n",
      "step 1100, training accuracy 0.851562, loss 0.435081\n",
      "test accuracy 0.699219\n",
      "step 1200, training accuracy 0.796875, loss 0.642931\n",
      "test accuracy 0.613281\n",
      "step 1300, training accuracy 0.820312, loss 0.52333\n",
      "test accuracy 0.65625\n",
      "step 1400, training accuracy 0.875, loss 0.404981\n",
      "test accuracy 0.671875\n",
      "step 1500, training accuracy 0.84375, loss 0.492685\n",
      "test accuracy 0.644531\n",
      "step 1600, training accuracy 0.796875, loss 0.540273\n",
      "test accuracy 0.714844\n",
      "step 1700, training accuracy 0.734375, loss 0.730314\n",
      "test accuracy 0.679688\n",
      "step 1800, training accuracy 0.898438, loss 0.38687\n",
      "test accuracy 0.707031\n",
      "step 1900, training accuracy 0.789062, loss 1.11155\n",
      "test accuracy 0.679688\n",
      "step 2000, training accuracy 0.90625, loss 0.304322\n",
      "test accuracy 0.707031\n",
      "step 2100, training accuracy 0.828125, loss 0.530097\n",
      "test accuracy 0.699219\n",
      "step 2200, training accuracy 0.84375, loss 0.478209\n",
      "test accuracy 0.707031\n",
      "step 2300, training accuracy 0.921875, loss 0.250134\n",
      "test accuracy 0.722656\n",
      "step 2400, training accuracy 0.914062, loss 0.326203\n",
      "test accuracy 0.664062\n",
      "step 2500, training accuracy 0.90625, loss 0.35472\n",
      "test accuracy 0.71875\n",
      "step 2600, training accuracy 0.867188, loss 0.360814\n",
      "test accuracy 0.601562\n",
      "step 2700, training accuracy 0.898438, loss 0.348189\n",
      "test accuracy 0.679688\n",
      "step 2800, training accuracy 0.929688, loss 0.33326\n",
      "test accuracy 0.683594\n",
      "step 2900, training accuracy 0.929688, loss 0.25253\n",
      "test accuracy 0.679688\n",
      "step 3000, training accuracy 0.828125, loss 0.558269\n",
      "test accuracy 0.671875\n",
      "step 3100, training accuracy 0.929688, loss 0.232569\n",
      "test accuracy 0.761719\n",
      "step 3200, training accuracy 0.890625, loss 0.31565\n",
      "test accuracy 0.667969\n",
      "step 3300, training accuracy 0.882812, loss 0.364435\n",
      "test accuracy 0.652344\n",
      "step 3400, training accuracy 0.960938, loss 0.172717\n",
      "test accuracy 0.6875\n",
      "step 3500, training accuracy 0.921875, loss 0.249203\n",
      "test accuracy 0.714844\n",
      "step 3600, training accuracy 0.953125, loss 0.197357\n",
      "test accuracy 0.632812\n",
      "step 3700, training accuracy 0.835938, loss 0.800609\n",
      "test accuracy 0.652344\n",
      "step 3800, training accuracy 0.882812, loss 0.673478\n",
      "test accuracy 0.714844\n",
      "step 3900, training accuracy 0.921875, loss 0.31499\n",
      "test accuracy 0.691406\n",
      "step 4000, training accuracy 0.929688, loss 0.264441\n",
      "test accuracy 0.710938\n",
      "step 4100, training accuracy 0.929688, loss 0.242063\n",
      "test accuracy 0.667969\n",
      "step 4200, training accuracy 0.976562, loss 0.130713\n",
      "test accuracy 0.679688\n",
      "step 4300, training accuracy 0.875, loss 0.245805\n",
      "test accuracy 0.671875\n",
      "step 4400, training accuracy 0.945312, loss 0.227435\n",
      "test accuracy 0.734375\n",
      "step 4500, training accuracy 0.929688, loss 0.317329\n",
      "test accuracy 0.710938\n",
      "step 4600, training accuracy 0.921875, loss 0.409316\n",
      "test accuracy 0.742188\n",
      "step 4700, training accuracy 0.921875, loss 0.235179\n",
      "test accuracy 0.703125\n",
      "step 4800, training accuracy 0.875, loss 0.530184\n",
      "test accuracy 0.664062\n",
      "step 4900, training accuracy 0.96875, loss 0.136848\n",
      "test accuracy 0.65625\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "for i in range(200):\n",
    "    batch = next_batch(256, x_train, y_train_one_hot.eval(session=sess))\n",
    "    train_step.run(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "    \n",
    "for i in range(max_steps):\n",
    "    batch = next_batch(128, x_train, y_train_one_hot.eval(session=sess))\n",
    "    #zero_one = x_drop2.eval(session=sess, feed_dict={x:batch[0]})\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "        loss = cross_entropy.eval(session = sess, feed_dict={ x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "\n",
    "        print('step %d, training accuracy %g, loss %g' % (i, train_accuracy, loss))\n",
    "            \n",
    "        test_batch = next_batch(256, x_test, y_test_one_hot.eval(session = sess))\n",
    "        print('test accuracy %g' % accuracy.eval(session = sess, feed_dict={x: test_batch[0], y: test_batch[1], keep_prob:1.0}))\n",
    "        #print('filter w ', filter_w1.eval(session=sess))\n",
    "        \n",
    "    train_step.run(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:0.2})\n",
    "    loss_val = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.84375, loss 0.46508\n",
      "test accuracy 0.699219\n",
      "step 100, training accuracy 0.921875, loss 0.257015\n",
      "test accuracy 0.707031\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-937a736ea517>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#print('filter w ', filter_w1.eval(session=sess))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   2211\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m     \"\"\"\n\u001b[0;32m-> 2213\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2215\u001b[0m \u001b[0m_gradient_registry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegistry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gradient\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   4791\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4792\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 4793\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4795\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "for i in range(max_steps):\n",
    "    batch = next_batch(128, x_train, y_train_one_hot.eval(session=sess))\n",
    "    #zero_one = x_drop2.eval(session=sess, feed_dict={x:batch[0]})\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "        loss = cross_entropy.eval(session = sess, feed_dict={ x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "\n",
    "        print('step %d, training accuracy %g, loss %g' % (i, train_accuracy, loss))\n",
    "            \n",
    "        test_batch = next_batch(256, x_test, y_test_one_hot.eval(session = sess))\n",
    "        print('test accuracy %g' % accuracy.eval(session = sess, feed_dict={x: test_batch[0], y: test_batch[1], keep_prob:1.0}))\n",
    "        #print('filter w ', filter_w1.eval(session=sess))\n",
    "        \n",
    "    train_step.run(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:0.2})\n",
    "    loss_val = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(200):\n",
    "    batch = next_batch(256, x_train, y_train_one_hot.eval(session=sess))\n",
    "    train_step.run(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "for i in range(max_steps):\n",
    "    batch = next_batch(128, x_train, y_train_one_hot.eval(session=sess))\n",
    "    #zero_one = x_drop2.eval(session=sess, feed_dict={x:batch[0]})\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "        loss = cross_entropy.eval(session = sess, feed_dict={ x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "\n",
    "        print('step %d, training accuracy %g, loss %g' % (i, train_accuracy, loss))\n",
    "            \n",
    "        test_batch = next_batch(256, x_test, y_test_one_hot.eval(session = sess))\n",
    "        print('test accuracy %g' % accuracy.eval(session = sess, feed_dict={x: test_batch[0], y: test_batch[1], keep_prob:1.0}))\n",
    "        #print('filter w ', filter_w1.eval(session=sess))\n",
    "        \n",
    "    train_step.run(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:0.2})\n",
    "    loss_val = loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
