{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.790845867\n",
      "Epoch: 0002 cost= 0.102035289\n",
      "Epoch: 0003 cost= 0.057962651\n",
      "Epoch: 0004 cost= 0.039800517\n",
      "Epoch: 0005 cost= 0.031437360\n",
      "Epoch: 0006 cost= 0.024450966\n",
      "Epoch: 0007 cost= 0.020721923\n",
      "Epoch: 0008 cost= 0.017076516\n",
      "Epoch: 0009 cost= 0.014268283\n",
      "Epoch: 0010 cost= 0.012788616\n",
      "Epoch: 0011 cost= 0.011049601\n",
      "Epoch: 0012 cost= 0.008858394\n",
      "Epoch: 0013 cost= 0.008332464\n",
      "Epoch: 0014 cost= 0.007924575\n",
      "Epoch: 0015 cost= 0.005868980\n",
      "Epoch: 0016 cost= 0.005727989\n",
      "Epoch: 0017 cost= 0.004683156\n",
      "Epoch: 0018 cost= 0.004355018\n",
      "Epoch: 0019 cost= 0.003867887\n",
      "Epoch: 0020 cost= 0.003676680\n",
      "Accuracy: 0.9897\n",
      "time:  90.62715816497803\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[3,3,1,32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[3,3,32,64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable(name=\"W3\", shape=[3,3,64,128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4 = tf.get_variable(name=\"W4\", shape=[3,3,128,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W5 = tf.get_variable(name=\"W5\", shape=[128, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "L1_c = tf.nn.conv2d(L, W1, strides=[1,1,1,1], padding='SAME') # ?, 28, 28, 32\n",
    "L1_r = tf.nn.relu(L1_c)\n",
    "L1_d = tf.nn.max_pool(L1_r, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ?, 14, 14, 32\n",
    "L1 = tf.nn.dropout(L1_d, dropout_rate)\n",
    "\n",
    "L2_c = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding='SAME') # ?, 14, 14, 64\n",
    "L2_r = tf.nn.relu(L2_c)\n",
    "L2_d = tf.nn.max_pool(L2_r, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ?, 7, 7, 64\n",
    "L2 = tf.nn.dropout(L2_d, dropout_rate)\n",
    "\n",
    "L3_c = tf.nn.conv2d(L2, W3, strides=[1,1,1,1], padding='SAME') # ?, 7, 7, 128\n",
    "L3_r = tf.nn.relu(L3_c)\n",
    "L3_d = tf.nn.max_pool(L3_r, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ?, 4, 4, 128\n",
    "L3 = tf.nn.dropout(L3_d, dropout_rate)\n",
    "\n",
    "col_w = tf.get_variable(name=\"col_sum\", shape=[4, 128], initializer=tf.ones_initializer())\n",
    "row_w = tf.get_variable(name=\"row_sum\", shape=[4, 128], initializer=tf.ones_initializer())\n",
    "\n",
    "col_sum = tf.reduce_sum(L3, axis=1)# * col_w\n",
    "row_sum = tf.reduce_sum(col_sum, axis=1)# * row_w\n",
    "#L3_sum = tf.concat([col_sum, row_sum], axis=1)\n",
    "\n",
    "L3_relu = tf.nn.relu(row_sum)\n",
    "L4_d = tf.reshape(L3_relu, [-1, 128])\n",
    "\n",
    "L5_result = tf.matmul(L4_d, W5)\n",
    "L5_d = tf.nn.dropout(L5_result, dropout_rate)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L5_d, labels=Y))\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\"\"\"session start\"\"\"\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X:batch_xs, Y:batch_ys, dropout_rate:1.0})\n",
    "            \n",
    "            avg_cost += c/total_batch\n",
    "\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        #print(col_w.eval())\n",
    "        #print(row_w.eval())\n",
    "    correct_prediction = tf.equal(tf.argmax(L5_d, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels, dropout_rate: 1.0}))\n",
    "    sess.close()\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting mnist/train-images-idx3-ubyte.gz\n",
      "Extracting mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting mnist/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost=  1.511669\n",
      "Epoch: 0002 cost=  1.434685\n",
      "Epoch: 0003 cost=  1.415413\n",
      "Epoch: 0004 cost=  1.379964\n",
      "Epoch: 0005 cost=  1.106680\n",
      "Epoch: 0006 cost=  0.971710\n",
      "Epoch: 0007 cost=  0.959732\n",
      "Epoch: 0008 cost=  0.954595\n",
      "Epoch: 0009 cost=  0.949953\n",
      "Epoch: 0010 cost=  0.947183\n",
      "Epoch: 0011 cost=  0.816800\n",
      "Epoch: 0012 cost=  0.725003\n",
      "Epoch: 0013 cost=  0.721102\n",
      "Epoch: 0014 cost=  0.716724\n",
      "Epoch: 0015 cost=  0.713692\n",
      "Accuracy: 0.6925\n",
      "time: 30.426372051239014\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Jan 29 20:48:23 2018\n",
    "\n",
    "@author: jsc5565\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import time\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"mnist\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.001\n",
    "train_epochs = 15\n",
    "train_size = 100\n",
    "display_step = 1\n",
    "filter_size = 20\n",
    "img_size = 14\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 28*28])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "row_w = tf.get_variable(name=\"row_w\", shape=[img_size, filter_size], initializer=tf.ones_initializer())\n",
    "col_w = tf.get_variable(name=\"col_w\", shape=[img_size, filter_size], initializer=tf.ones_initializer())\n",
    "filter_w = tf.Variable(tf.random_normal([7,7,1,filter_size], stddev=0.01))\n",
    "output = tf.get_variable(name=\"output\", shape=[img_size*filter_size, 10], initializer=tf.ones_initializer())\n",
    "logit = tf.get_variable(name=\"logit\", shape=[img_size*img_size, 10], initializer=tf.ones_initializer())\n",
    "\n",
    "w = tf.get_variable(name=\"w\", shape=[14*14*filter_size, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name=\"b\", shape=[10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "x_re = tf.reshape(x, [-1, 28, 28, 1])\n",
    "x_conv = tf.nn.conv2d(x_re, filter_w, strides=[1,1,1,1], padding='SAME')\n",
    "x_pool = tf.nn.max_pool(x_conv, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') #?, 14, 14, filter_size\n",
    "\n",
    "\"\"\"first conv2d\"\"\"\n",
    "\"\"\"\n",
    "mul1 = x_pool #* row_w\n",
    "transpose = tf.transpose(mul1, perm=[0, 2, 1, 3])\n",
    "mul2 = transpose# * col_w\n",
    "\"\"\"\n",
    "\n",
    "col_sum = tf.reduce_sum(x_pool, axis=1) * row_w\n",
    "row_sum = tf.reduce_sum(x_pool, axis=2) * col_w\n",
    "\n",
    "col_sum_re = tf.reshape(col_sum, [-1, img_size*filter_size])\n",
    "row_sum_re = tf.reshape(row_sum, [-1, img_size*filter_size])\n",
    "\n",
    "total_sum = col_sum_re + row_sum_re\n",
    "relu_add = tf.nn.relu(total_sum)\n",
    "\n",
    "#esult = tf.matmul(relu_add, output)\n",
    "\n",
    "result = tf.matmul(tf.reshape(x_pool,[-1, 14*14*filter_size]), w) + b\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=result, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "\n",
    "\"\"\"sesson start\"\"\"\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    start_time = time.time()\n",
    "    for epoch in range(train_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        total_epochs = int(mnist.train.num_examples/ train_size)\n",
    "        \n",
    "        for i in range(total_epochs):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(train_size)\n",
    "            _,c, row= sess.run([optimizer, cost, row_w], {x: batch_xs, y: batch_ys})\n",
    "            avg_cost += c/total_epochs\n",
    "            \n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:9f}\".format(avg_cost))\n",
    "            \n",
    "    prediction = tf.equal(tf.argmax(result, 1), tf.argmax(y, 1))            \n",
    "            \n",
    "    accuracy = tf.reduce_mean(tf.cast(prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y:mnist.test.labels}))\n",
    "    \n",
    "    sess.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print('time:', end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.742268469\n",
      "Epoch: 0002 cost= 0.088006752\n",
      "Epoch: 0003 cost= 0.055067030\n",
      "Epoch: 0004 cost= 0.040490197\n",
      "Epoch: 0005 cost= 0.031381931\n",
      "Epoch: 0006 cost= 0.025898498\n",
      "Epoch: 0007 cost= 0.020996972\n",
      "Epoch: 0008 cost= 0.017124703\n",
      "Epoch: 0009 cost= 0.014099406\n",
      "Epoch: 0010 cost= 0.012656907\n",
      "Epoch: 0011 cost= 0.011152675\n",
      "Epoch: 0012 cost= 0.009004985\n",
      "Epoch: 0013 cost= 0.007741175\n",
      "Epoch: 0014 cost= 0.007477674\n",
      "Epoch: 0015 cost= 0.005919517\n",
      "Epoch: 0016 cost= 0.005724914\n",
      "Epoch: 0017 cost= 0.005305135\n",
      "Epoch: 0018 cost= 0.004476333\n",
      "Epoch: 0019 cost= 0.003892988\n",
      "Epoch: 0020 cost= 0.003451465\n",
      "Accuracy: 0.9908\n",
      "time:  89.19786977767944\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 20\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "\n",
    "W1 = tf.get_variable(name=\"W1\", shape=[3,3,1,32], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(name=\"W2\", shape=[3,3,32,64], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable(name=\"W3\", shape=[3,3,64,128], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4 = tf.get_variable(name=\"W4\", shape=[3,3,128,256], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W5 = tf.get_variable(name=\"W5\", shape=[128, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L = tf.reshape(X, [-1, 28, 28, 1])\n",
    "\n",
    "L1_c = tf.nn.conv2d(L, W1, strides=[1,1,1,1], padding='SAME') # ?, 28, 28, 32\n",
    "L1_r = tf.nn.relu(L1_c)\n",
    "L1_d = tf.nn.max_pool(L1_r, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ?, 14, 14, 32\n",
    "L1 = tf.nn.dropout(L1_d, dropout_rate)\n",
    "\n",
    "L2_c = tf.nn.conv2d(L1, W2, strides=[1,1,1,1], padding='SAME') # ?, 14, 14, 64\n",
    "L2_r = tf.nn.relu(L2_c)\n",
    "L2_d = tf.nn.max_pool(L2_r, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ?, 7, 7, 64\n",
    "L2 = tf.nn.dropout(L2_d, dropout_rate)\n",
    "\n",
    "L3_c = tf.nn.conv2d(L2, W3, strides=[1,1,1,1], padding='SAME') # ?, 7, 7, 128\n",
    "L3_r = tf.nn.relu(L3_c)\n",
    "L3_d = tf.nn.max_pool(L3_r, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') # ?, 4, 4, 128\n",
    "L3_dr = tf.nn.max_pool(L3_d, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "L3_dr2 = tf.nn.max_pool(L3_dr, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "L3_sq = tf.squeeze(L3_dr2)\n",
    "#L3 = tf.nn.dropout(L3_d, dropout_rate)\n",
    "\n",
    "#L4_d = tf.reshape(L3, [-1, 128])\n",
    "\n",
    "L5_result = tf.matmul(L3_sq, W5)\n",
    "L5_d = tf.nn.dropout(L5_result, dropout_rate)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=L5_d, labels=Y))\n",
    "optimizer = tf.train.RMSPropOptimizer(0.001, 0.9).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\"\"\"session start\"\"\"\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X:batch_xs, Y:batch_ys, dropout_rate:1.0})\n",
    "            \n",
    "            avg_cost += c/total_batch\n",
    "\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        #print(col_w.eval())\n",
    "        #print(row_w.eval())\n",
    "    correct_prediction = tf.equal(tf.argmax(L5_d, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels, dropout_rate: 1.0}))\n",
    "    sess.close()\n",
    "    \n",
    "end_time = time.time()\n",
    "\n",
    "print(\"time: \", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.315742742\n",
      "Epoch: 0002 cost= 0.147040928\n",
      "Epoch: 0003 cost= 0.094361581\n",
      "Epoch: 0004 cost= 0.065167977\n",
      "Epoch: 0005 cost= 0.046750932\n",
      "Epoch: 0006 cost= 0.033143676\n",
      "Epoch: 0007 cost= 0.024050335\n",
      "Epoch: 0008 cost= 0.016104399\n",
      "Epoch: 0009 cost= 0.012524754\n",
      "Epoch: 0010 cost= 0.009971890\n",
      "Epoch: 0011 cost= 0.006657040\n",
      "Epoch: 0012 cost= 0.004595839\n",
      "Epoch: 0013 cost= 0.003418774\n",
      "Epoch: 0014 cost= 0.002177531\n",
      "Epoch: 0015 cost= 0.010395858\n",
      "Epoch: 0016 cost= 0.005931697\n",
      "Epoch: 0017 cost= 0.002465400\n",
      "Epoch: 0018 cost= 0.000599638\n",
      "Epoch: 0019 cost= 0.000310317\n",
      "Epoch: 0020 cost= 0.000229364\n",
      "Epoch: 0021 cost= 0.000185155\n",
      "Epoch: 0022 cost= 0.000155940\n",
      "Epoch: 0023 cost= 0.000138171\n",
      "Epoch: 0024 cost= 0.022205791\n",
      "Epoch: 0025 cost= 0.002030770\n",
      "Epoch: 0026 cost= 0.000378477\n",
      "Epoch: 0027 cost= 0.000238196\n",
      "Epoch: 0028 cost= 0.000186633\n",
      "Epoch: 0029 cost= 0.000151895\n",
      "Epoch: 0030 cost= 0.000124835\n",
      "Accuracy: 0.984\n",
      "time: 39.527034282684326\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import numpy as np\n",
    "import time\n",
    "tf.reset_default_graph()\n",
    "\n",
    "mnist = input_data.read_data_sets('./mnist/data', one_hot=True)\n",
    "\n",
    "batch_size = 100\n",
    "training_epochs = 30\n",
    "learning_rate = 0.001\n",
    "display_step = 1\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "\n",
    "w = tf.get_variable(name=\"w\", shape=[784, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name=\"b\", shape=[1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "output_w = tf.get_variable(name=\"output_w\", shape=[1024, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def Activation(matrix):\n",
    "    output_save = 1.1/(1+tf.exp(-3*matrix))-0.05\n",
    "    #output_save = 1/tf.tanh(matrix)\n",
    "    output_max = tf.minimum(1.0, output_save)\n",
    "    output_min = tf.maximum(-1.0, output_max)\n",
    "    return output_save\n",
    "\n",
    "\n",
    "L1 = Activation(tf.matmul(x, w)+b)\n",
    "#L1 = tf.nn.relu(tf.matmul(x,w)+b)\n",
    "result = tf.matmul(L1, output_w)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=result, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {x:batch_xs, y:batch_ys})\n",
    "            \n",
    "            avg_cost += c/total_batch\n",
    "\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "            \n",
    "        if avg_cost < 0.01:\n",
    "            learning_rate = 0.0001\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(result, 1), tf.argmax(y, 1))\n",
    "        \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({x: mnist.test.images, y: mnist.test.labels}))\n",
    "    sess.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#learning_rate = 0.01, cost= 7.652248363, Accuracy: 0.9675\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.05\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "\n",
    "W1 = tf.get_variable(name=\"w1\", shape=[784, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(name=\"w2\", shape=[1024, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W3 = tf.get_variable(name=\"w3\", shape=[1024, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W4 = tf.get_variable(name=\"w4\", shape=[1024, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W5 = tf.get_variable(name=\"w5\", shape=[784*784, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "B1 = tf.get_variable(name=\"b1\", shape=[1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2 = tf.get_variable(name=\"b2\", shape=[1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B3 = tf.get_variable(name=\"b3\", shape=[1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B4 = tf.get_variable(name=\"b4\", shape=[1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B5 = tf.get_variable(name=\"b5\", shape=[10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def Activation(matrix):\n",
    "    #output_save = 1.1/(1+tf.exp(-3*matrix))-0.05\n",
    "    #output_save = 2/(1+tf.exp(-3*matrix))\n",
    "    output_save = matrix\n",
    "    #output_max = tf.minimum(1.0, output_save)\n",
    "    output_min = tf.maximum(0.0, output_save)\n",
    "    return output_min\n",
    "\"\"\"\n",
    "L1 = Activation(tf.add(tf.matmul(X, W1), B1))\n",
    "L2 = Activation(tf.add(tf.matmul(L1, W2), B2))\n",
    "L3 = Activation(tf.add(tf.matmul(L2, W3), B3))\n",
    "L4 = Activation(tf.add(tf.matmul(L3, W4), B4))\n",
    "result = tf.add(tf.matmul(L4, W5), B5)\n",
    "\"\"\"\n",
    "prev = tf.reshape(tf.matmul(tf.reshape(X, [-1, 784, 1]), tf.reshape(X, [-1, 1, 784])), [-1, 784*784])\n",
    "#result = tf.add(tf.matmul(tf.matmul(X, tf.reshape(X,[-1, 784])),W5), B5)\n",
    "result = tf.add(tf.matmul(prev, W5), B5)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=result, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "config = tf.ConfigProto(log_device_placement=True)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(init)\n",
    "    start_time = time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X:batch_xs, Y:batch_ys})\n",
    "            \n",
    "            avg_cost += c/total_batch\n",
    "\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(result, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    sess.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist/data/train-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/train-labels-idx1-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./mnist/data/t10k-labels-idx1-ubyte.gz\n",
      "Epoch: 0001 cost= 0.230417258\n",
      "Epoch: 0002 cost= 0.125297891\n",
      "Epoch: 0003 cost= 0.098196880\n",
      "Epoch: 0004 cost= 0.094499739\n",
      "Epoch: 0005 cost= 0.079659247\n",
      "Epoch: 0006 cost= 0.069341511\n",
      "Epoch: 0007 cost= 0.068506377\n",
      "Epoch: 0008 cost= 0.070406580\n",
      "Epoch: 0009 cost= 0.059581596\n",
      "Epoch: 0010 cost= 0.051555238\n",
      "Epoch: 0011 cost= 0.051955377\n",
      "Epoch: 0012 cost= 0.055439913\n",
      "Epoch: 0013 cost= 0.060869519\n",
      "Epoch: 0014 cost= 0.052048217\n",
      "Epoch: 0015 cost= 0.040070906\n",
      "Accuracy: 0.9691\n",
      "time: 18.156053066253662\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#learning_rate = 0.01, cost= 7.652248363, Accuracy: 0.9675\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "\n",
    "mnist = input_data.read_data_sets(\"./mnist/data/\", one_hot=True)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "learning_rate = 0.01\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "display_step = 1\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "Y = tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "dropout_rate = tf.placeholder(\"float\")\n",
    "\n",
    "W1 = tf.get_variable(name=\"w1\", shape=[784, 1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "W2 = tf.get_variable(name=\"w2\", shape=[1024, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "B1 = tf.get_variable(name=\"b1\", shape=[1024], initializer=tf.contrib.layers.xavier_initializer())\n",
    "B2 = tf.get_variable(name=\"b2\", shape=[10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "L1 = tf.nn.relu(tf.add(tf.matmul(X, W1), B1))\n",
    "result = tf.add(tf.matmul(L1, W2), B2)\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=result, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=0.333)\n",
    "\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=gpu_options)) as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0\n",
    "        \n",
    "        total_batch = int(mnist.train.num_examples/batch_size)\n",
    "        \n",
    "        for i in range(total_batch):\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "            _, c = sess.run([optimizer, cost], feed_dict = {X:batch_xs, Y:batch_ys})\n",
    "            \n",
    "            avg_cost += c/total_batch\n",
    "\n",
    "        if (epoch+1) % display_step == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost))\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(result, 1), tf.argmax(Y, 1))\n",
    "        \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    print(\"Accuracy:\", accuracy.eval({X: mnist.test.images, Y: mnist.test.labels}))\n",
    "    sess.close()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"time:\", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
