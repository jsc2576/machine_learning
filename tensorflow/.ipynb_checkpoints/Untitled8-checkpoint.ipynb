{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "from tensorflow.python.keras._impl.keras.datasets.cifar10 import load_data\n",
    "tf = load_data()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    \n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = load_data()\n",
    "\n",
    "# scalar 형태의 레이블(0~9)을 One-hot Encoding 형태로 변환한다.\n",
    "y_train_one_hot = tf.squeeze(tf.one_hot(y_train, 10),axis=1)\n",
    "y_test_one_hot = tf.squeeze(tf.one_hot(y_test, 10),axis=1)\n",
    "\n",
    "# Input과 Ouput의 차원을 가이드한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_steps = 3001\n",
    "filter_size = 9*9*9\n",
    "learning_rate = 0.001\n",
    "display_step = 1\n",
    "img_size = 16\n",
    "fully_size = 256\n",
    "\n",
    "filter1_size = 16\n",
    "filter2_size = 100\n",
    "filter3_size = 200\n",
    "filter4_size = 400\n",
    "\n",
    "learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 32, 32, 3])\n",
    "y = tf.placeholder(tf.float32, [None, 10])\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "\n",
    "row_w = tf.get_variable(name=\"row_w\", shape=[img_size, filter_size], initializer=tf.ones_initializer())\n",
    "col_w = tf.get_variable(name=\"col_w\", shape=[img_size, filter_size], initializer=tf.ones_initializer())\n",
    "output = tf.get_variable(name=\"output\", shape=[img_size*filter_size, 10], initializer=tf.ones_initializer())\n",
    "logit = tf.get_variable(name=\"logit\", shape=[img_size*img_size, 10], initializer=tf.ones_initializer())\n",
    "\n",
    "w = tf.get_variable(name=\"w\", shape=[fully_size, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b = tf.get_variable(name=\"b\", shape=[10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "filter_w1 = tf.get_variable(name=\"filter_w1\", shape=[2,2,3,filter1_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "filter_w2 = tf.get_variable(name=\"filter_w2\", shape=[5,5,filter1_size, filter2_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "filter_w3 = tf.get_variable(name=\"filter_w3\", shape=[4,4,filter2_size, filter3_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "filter_w4 = tf.get_variable(name=\"filter_w4\", shape=[4,4,filter3_size, filter4_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "x_conv1 = tf.nn.conv2d(x, filter_w1, strides=[1,1,1,1], padding='SAME')\n",
    "\n",
    "x_conv2 = tf.nn.conv2d(tf.nn.relu(x_conv1), filter_w2, strides=[1,1,1,1], padding='SAME')\n",
    "x_relu2 = tf.nn.relu(x_conv2)\n",
    "x_max_pool2 = tf.nn.max_pool(x_relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') #?, 16,16,filter2_size\n",
    "x_max_pool2_drop = tf.nn.dropout(x_max_pool2, keep_prob=keep_prob)\n",
    "\n",
    "\"\"\"\n",
    "x_min_pool2_rev = tf.nn.max_pool(-x_relu2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "x_min_pool2 = -x_min_pool2_rev\n",
    "x_min_pool2_drop = tf.nn.dropout(x_min_pool2, keep_prob=keep_prob)\n",
    "x_concat2 = tf.concat([x_max_pool2_drop, x_min_pool2_drop], axis=3)\n",
    "print(x_concat2.shape)\n",
    "\"\"\"\n",
    "x_conv3 = tf.nn.conv2d(x_max_pool2_drop, filter_w3, strides=[1,1,1,1], padding='SAME')\n",
    "x_relu3 = tf.nn.relu(x_conv3)\n",
    "x_max_pool3 = tf.nn.max_pool(x_relu3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') #?, 8,8,filter3_size\n",
    "x_max_pool3_drop = tf.nn.dropout(x_max_pool3, keep_prob=keep_prob)\n",
    "\"\"\"\n",
    "x_min_pool3_rev = tf.nn.max_pool(-x_relu3, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "x_min_pool3 = -x_min_pool3_rev\n",
    "x_min_pool3_drop = tf.nn.dropout(x_min_pool3, keep_prob=keep_prob)\n",
    "x_concat3 = tf.concat([x_max_pool3_drop, x_min_pool3_drop], axis=3)\n",
    "print(x_conv3.shape)\n",
    "print(x_concat3.shape)\n",
    "\"\"\"\n",
    "x_conv4 = tf.nn.conv2d(x_max_pool3_drop, filter_w4, strides=[1,1,1,1], padding='SAME')\n",
    "x_relu4 = tf.nn.relu(x_conv4)\n",
    "x_max_pool4 = tf.nn.max_pool(x_relu4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME') #?, 4,4,filter4_size\n",
    "#x_max_pool4_drop = tf.nn.dropout(x_max_pool4, keep_prob=keep_prob)\n",
    "\"\"\"\n",
    "x_min_pool4_rev = tf.nn.max_pool(-x_relu4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "x_min_pool4 = -x_min_pool4_rev\n",
    "x_min_pool4_drop = tf.nn.dropout(x_min_pool4, keep_prob=keep_prob)\n",
    "x_concat4 = tf.concat([x_max_pool4_drop, x_min_pool4_drop], axis=3)\n",
    "print(x_conv4.shape)\n",
    "print(x_concat4.shape)\n",
    "\"\"\"\n",
    "w_pool = tf.get_variable(name=\"w_pool\", shape=[4*4*filter4_size, fully_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "b_pool = tf.get_variable(name=\"b_pool\", shape=[fully_size], initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "x_pool_re = tf.reshape(x_max_pool4, [-1, 4*4*filter4_size])\n",
    "fully = tf.nn.relu(tf.add(tf.matmul(x_pool_re, w_pool), b_pool))\n",
    "\n",
    "result = tf.matmul(fully, w) + b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-c2265ec041da>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=result)) \n",
    "train_step = tf.train.RMSPropOptimizer(learning_rate = learning_rate).minimize(cross_entropy)\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(result, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.101562, loss 13.4875\n",
      "test accuracy 0.1002\n",
      "step 100, training accuracy 0.304688, loss 2.17635\n",
      "test accuracy 0.3684\n",
      "step 200, training accuracy 0.617188, loss 1.18363\n",
      "test accuracy 0.5609\n",
      "step 300, training accuracy 0.609375, loss 1.32123\n",
      "test accuracy 0.5998\n",
      "step 400, training accuracy 0.648438, loss 1.21003\n",
      "test accuracy 0.654\n",
      "step 500, training accuracy 0.789062, loss 0.747436\n",
      "test accuracy 0.6618\n",
      "step 600, training accuracy 0.710938, loss 0.925261\n",
      "test accuracy 0.6914\n",
      "step 700, training accuracy 0.796875, loss 0.701399\n",
      "test accuracy 0.6961\n",
      "step 800, training accuracy 0.726562, loss 0.971889\n",
      "test accuracy 0.6845\n",
      "step 900, training accuracy 0.8125, loss 0.601009\n",
      "test accuracy 0.6851\n",
      "step 1000, training accuracy 0.796875, loss 0.946859\n",
      "test accuracy 0.699\n",
      "step 1100, training accuracy 0.742188, loss 0.964909\n",
      "test accuracy 0.706\n",
      "step 1200, training accuracy 0.835938, loss 0.422968\n",
      "test accuracy 0.7002\n",
      "step 1300, training accuracy 0.804688, loss 0.635371\n",
      "test accuracy 0.7053\n",
      "step 1400, training accuracy 0.859375, loss 0.416167\n",
      "test accuracy 0.72\n",
      "step 1500, training accuracy 0.742188, loss 1.02283\n",
      "test accuracy 0.7081\n",
      "step 1600, training accuracy 0.773438, loss 0.852593\n",
      "test accuracy 0.7109\n",
      "step 1700, training accuracy 0.789062, loss 0.86733\n",
      "test accuracy 0.7326\n",
      "step 1800, training accuracy 0.789062, loss 0.628416\n",
      "test accuracy 0.7146\n"
     ]
    }
   ],
   "source": [
    "for i in range(max_steps):\n",
    "    batch = next_batch(128, x_train, y_train_one_hot.eval(session=sess))\n",
    "    \n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "        loss = cross_entropy.eval(session = sess, feed_dict={ x: batch[0], y: batch[1], keep_prob:1.0})\n",
    "\n",
    "        print('step %d, training accuracy %g, loss %g' % (i, train_accuracy, loss))\n",
    "            \n",
    "        test_accuracy = 0\n",
    "        \n",
    "        for j in range(10):\n",
    "            test_accuracy += accuracy.eval(session = sess, feed_dict={x: x_test[1000*j:1000*(j+1)], y: y_test_one_hot.eval(session = sess)[1000*j:1000*(j+1)], keep_prob:1.0})\n",
    "        test_accuracy /= 10\n",
    "        print('test accuracy %g' % test_accuracy)\n",
    "        #print('filter w ', filter_w1.eval(session=sess))\n",
    "        \n",
    "    for j in range(4):\n",
    "        train_step.run(session = sess, feed_dict={x: batch[0], y: batch[1], keep_prob:0.7})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
